# `components.models`
[source](https://github.com/Health-Informatics-UoN/lettuce/tree/main/lettuce/components/models.py)

This module provides functionality to load the user specified model as part of the `components.pipeline.LLMPipeline`. 
It supports both local model inference via llama.cpp or Ollama and remote inference via OpenAI's API.

The module manages LLM initialisation and selection to power the source term standardisation pipeline, with support for:
- Local model inference using quantized GGUF models
- Remote inference using OpenAI models
- Automatic model downloading from Hugging Face Hub
- Ollama server integration for local model serving

## Functions 

### `get_local_weights`
```python
def get_local_weights(
    path_to_weights: os.PathLike | str | None, 
    temperature: float, 
    logger: logging.Logger,
    verbose: bool
) -> LlamaCppGenerator:
```

Load a local GGUF model weights file and return a LlamaCppGenerator object.

#### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `path_to_weights` | `os.PathLike \| str \| None` | The full path to the local GGUF model weights file (e.g., "/path/to/llama-2-7b-chat.Q4_0.gguf"). |
| `temperature` | `float` | The temperature for model generation |
| `logger` | `logging.Logger` | Logger instance for tracking progress and errors. |
| `verbose` | `bool` | If true, the generator logs information about loading weights and generation |

#### Returns
`LlamaCppGenerator`

A loaded LlamaCppGenerator object ready for inference.

#### Raises
`FileNotFoundError`

If the specified file_path does not exist or is not a file.

### `download_model_from_huggingface`
```python
def download_model_from_huggingface(
    model_name: str, 
    temperature: float, 
    logger: logging.Logger, 
    verbose: bool,
    fallback_model: str = "llama-3.1-8b",
    n_ctx: int = 1024,
    n_batch: int = 32,
    max_tokens: int = 128 
) -> LlamaCppGenerator:
```

Load GGUF model weights from a hugging face repository.

#### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model_name` | `str` | The name of a model with repository details in the local_models dictionary |
| `temperature` | `float` | The temperature for model generation |
| `logger` | `logging.Logger` | Logger instance for tracking progress and errors. |
| `verbose` | `bool` | If true, the generator logs information about loading weights and generation |
| `fallback_model` | `str` | If the model name that's specified is not in the local_models dictionary, loads this one instead. Defaults to llama-3.1-8b |
| `n_ctx` | `int` | Context size for the model |
| `n_batch` | `int` | Number of tokens sent to the model in each batch. Defaults to 32 |
| `max_tokens` | `int` | Maximum tokens to generate. Defaults to 128. |

#### Returns
`LlamaCppGenerator`

A loaded LlamaCppGenerator object ready for inference.

#### Raises
`ValueError`

If the model fails to download or initialize.

### `connect_to_openai`
```python
def connect_to_openai(
    model_name: str, 
    temperature: float, 
    logger: logging.Logger,
) -> OpenAIGenerator:
```

Connect to OpenAI API and return an OpenAIGenerator object.

#### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model_name` | `str` | The name of the OpenAI model (e.g., "gpt-4", "gpt-3.5-turbo") |
| `temperature` | `float` | The temperature for model generation |
| `logger` | `logging.Logger` | Logger instance for tracking progress and errors. |

#### Returns
`OpenAIGenerator`

A configured OpenAIGenerator object for API-based inference.

### `connect_to_ollama`
```python
def connect_to_ollama(
    model_name: str,
    url: str,
    temperature: float,
    logger: logging.Logger,
    max_tokens: int = 128,
) -> OllamaGenerator:
```

Connect to an Ollama server and return an OllamaGenerator object.

#### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model_name` | `str` | The name of the Ollama model to use |
| `url` | `str` | The URL of the Ollama server |
| `temperature` | `float` | The temperature for model generation |
| `logger` | `logging.Logger` | Logger instance for tracking progress and errors. |
| `max_tokens` | `int` | Maximum number of tokens to generate. Defaults to 128. |

#### Returns
`OllamaGenerator`

A configured OllamaGenerator object for Ollama server-based inference.

#### Raises
`Exception`

If connection to the Ollama server fails or the model is not available.

### `get_model`
```python
def get_model(
    model: LLMModel, 
    logger: logging.Logger, 
    inference_type: InferenceType,
    url: str,
    temperature: float = 0.7, 
    path_to_local_weights: os.PathLike[Any] | str | None = None,
    verbose: bool = False,
) -> OpenAIGenerator | LlamaCppGenerator | OllamaGenerator:
```

Get an interface for interacting with an LLM. 

If a path to a `.gguf` model file is provided via `path_to_local_weights`, the model is loaded locally using a `LlamaCppGenerator`. In this case, no remote download or API requests will take place.

If no local path is provided, the function uses Haystack Generators to provide an interface to a model based on the `inference_type`:
- `OPEN_AI`: Creates an interface to a remote OpenAI model
- `OLLAMA`: Creates an interface to an Ollama server
- Other types: Uses a `LlamaCppGenerator` to start a llama.cpp model and provide an interface

#### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | `LLMModel` | An enum representing the desired model. The enum's value should match one of the registered model names (e.g., `"llama-3.1-8b"` or `"gpt-4"`). |
| `logger` | `logging.Logger` | Logger instance for tracking progress and errors. |
| `inference_type` | `InferenceType` | Whether to use Llama.cpp, Ollama, or the OpenAI API for inference |
| `url` | `str` | The URL for the Ollama server (only used when inference_type is OLLAMA) |
| `temperature` | `float` | Controls the randomness of the output. Higher values (e.g., 1.0) make output more diverse, while lower values (e.g., 0.2) make it more deterministic. Defaults to 0.7. |
| `path_to_local_weights` | `os.PathLike[Any] \| str \| None` | Path to a local `.gguf` model weights file. If provided, the function skips remote model loading and uses this local file. If not provided, the function will attempt to load the model from Hugging Face or connect to OpenAI. |
| `verbose` | `bool` | If true, the generator logs information about loading weights and generation. Defaults to False |

#### Returns
`OpenAIGenerator | LlamaCppGenerator | OllamaGenerator`

A LLM text generation interface compatible with Haystack's component framework.

## Implemented models

| Model name | Summary |
|:-----------|--------:|
|[llama-3.1-8b](https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF)| **Recommended** Meta's Llama 3.1 with 8 billion parameters, quantized to 4 bits|
|[llama-2-7b-chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)| Meta's Llama 2 with 7 billion parameters, quantized to 4 bits |
|[llama-3-8b](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF-v2)| Meta's Llama 3 with 8 billion parameters, quantized to 4 bits |
|[llama-3-70b](https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF-v2)| Meta's Llama 3 with 70 billion parameters, quantized to 4 bits| 
|[gemma-7b](https://huggingface.co/MaziyarPanahi/gemma-7b-GGUF)| Google's Gemma with 7 billion parameters, quantized to 4 bits |
|[llama-3.2-3b](https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF)| Meta's Llama 3.2 with 3 billion parameters, quantized to 6 bits|
|[mistral-7b](https://huggingface.co/TheBloke/Mistral-7B-GGUF)| Mistral at 7 billion parameters, quantized to 4 bits |
|[kuchiki-l2-7b](https://huggingface.co/TheBloke/Kuchiki-L2-7B-GGUF)| A merge of several models at 7 billion parameters, quantized to 4 bits|
|[tinyllama-1.1b-chat](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF)| Llama 2 extensively pre-trained, with 1.1 billion parameters, quantized to 4 bits|
|[biomistral-7b](https://huggingface.co/MaziyarPanahi/BioMistral-7B-GGUF)| Mistral at 7 billion parameters, pre-trained on biomedical data, quantized to 4 bits|
|[qwen2.5-3b-instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF)| Alibaba's Qwen 2.5 at 3 billion parameters, quantized to 5 bits |
|[airoboros-3b](https://huggingface.co/afrideva/airoboros-3b-3p0-GGUF)| Llama 2 pre-trained on the airoboros 3.0 dataset at 3 billion parameters, quantized to 4 bits |
|[medicine-chat](https://huggingface.co/TheBloke/medicine-chat-GGUF)| Llama 2 pre-trained on medical data, quantized to 4 bits |
|[medicine-llm-13b](https://huggingface.co/TheBloke/medicine-LLM-13B-GGUF)| Llama pre-trained on medical data at 13 billion parameters, quantized to 3 bits|
|[med-llama-3-8b-v1](https://huggingface.co/bartowski/JSL-MedLlama-3-8B-v1.0-GGUF)| Llama 3 at 8 billion parameters, pre-trained on medical data, quantized to 5 bits |
|[med-llama-3-8b-v2](https://huggingface.co/bartowski/JSL-MedLlama-3-8B-v1.0-GGUF)| Llama 3 at 8 billion parameters, pre-trained on medical data, quantized to 4 bits |
|[med-llama-3-8b-v3](https://huggingface.co/bartowski/JSL-MedLlama-3-8B-v1.0-GGUF)| Llama 3 at 8 billion parameters, pre-trained on medical data, quantized to 3 bits |
|[med-llama-3-8b-v4](https://huggingface.co/bartowski/JSL-MedLlama-3-8B-v1.0-GGUF)| Llama 3 at 8 billion parameters, pre-trained on medical data, quantized to 3 bits |

If you would like to add a model, [raise an issue](https://github.com/Health-Informatics-UoN/lettuce/issues)
