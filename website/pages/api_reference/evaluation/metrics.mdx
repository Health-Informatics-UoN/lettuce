## evaluation.metrics
[source](https://github.com/Health-Informatics-UoN/lettuce/tree/main/Lettuce/evaluation/metrics.py)
### `ExactMatch`
```python
class ExactMatch()
```

A metric checking whether the predicted and desired output match.
This doesn't care what the inputs are.
#### Methods

##### `__init__`
```
def __init__()
```



##### `calculate`
```python
def calculate(
	predicted: 
	actual: 
)
```

Calculate the exact match metric.

###### Parameters
`predicted`
The predicted output from the pipeline.

`actual`
The desired output.

###### Returns
1 if the predicted and actual outputs match exactly, 0 otherwise.



##### `description`
```python
def description()
```


### `UncasedMatch`
```python
class UncasedMatch()
```

A metric for testing whether the predicted and desired outputs are matching strings.
Case-insensitive and strips whitespace.
#### Methods

##### `__init__`
```python
def __init__()
```



##### `calculate`
```python
def calculate(
	predicted: str
	actual: str
)
```

Calculate the exact match metric, if the input value has been wrapped in a list

###### Parameters
`predicted`
The predicted output from the pipeline

`actual: list`
A list where an item is the desired output of the pipeline

###### Returns
1 if the predicted and actual outputs match exactly, 0 otherwise



##### `description`
```python
def description()
```


### `FuzzyMatchRatio`
```python
class FuzzyMatchRatio()
```

A metric that compares predicted strings to desired output.

Scores are normalised InDel distance
#### Methods

##### `__init__`
```python
def __init__()
```



##### `calculate`
```python
def calculate(
	predicted: str
	actual: str
)
```

Calculates the Fuzzy Match Ratio metric

###### Parameters
`predicted: str`
    String output from a SingleResultPipeline
`actual: str`
    Ground truth, the string the pipeline is trying to predict

##### `description`
```python
def description()
```


### `calc_precision`
```python
def calc_precision(
	relevant_instances: Any[List]
	prediction: Any[List]
)
```

Compares two lists and calculates precision

$Precision = \frac{Number\ of\ relevant\ instances\ retrieved}{Number\ of\ instances\ retrieved}$

#### Parameters
`relevant_instances: List[Any]`
The set of relevant instances, or positive class

`prediction: List[Any]`
A prediction made by an information retrieval system

##### Returns
`float`

A score for the precision


### `calc_recall`
```python
def calc_recall(
	relevant_instances: List[Any]
	prediction: List[Any]
)
```

Compares two lists and calculates recall

$Recall = \frac{Number\ of\ relevant\ instances\ retrieved}{Number\ of\ relevant\ instances}$

#### Parameters
`relevant_instances: List[Any]`
The set of relevant instances, or positive class

`prediction: List[Any]`
A prediction made by an information retrieval system

#### Returns
`float`
A score for the recall


### `PrecisionMetric`
```python
class PrecisionMetric()
```


#### Methods

##### `__init__`
```
def __init__()
```



##### `calculate`
```python
def calculate(
	predicted: List[Any]
	actual: List[Any]
)
```

Calculates precision for the information retrieval pipeline's prediction against a positive set

###### Parameters
`predicted: List[Any]`
    The output of an information retrieval pipeline

`actual: List[Any]`
    The set of relevant instances for the input

##### `description`
```python
def description()
```


### `RecallMetric`
```python
class RecallMetric()
```


#### Methods

##### `__init__`
```python
def __init__()
```

##### `calculate`
```python
def calculate(
	predicted: List[Any]
	actual: List[Any]
)
```

Calculates recall for the information retrieval pipeline's prediction against a positive set

###### Parameters
`predicted: List[Any]`
    The output of an information retrieval pipeline

`actual: List[Any]`
    The set of relevant instances for the input

##### `description`
```python
def description()
```


### `FScoreMetric`
```python
class FScoreMetric(
	beta: float
)
```


#### Methods

##### `__init__`
```python
def __init__(
	beta: float
)
```

Initialises the F-Score metric

###### Parameters
`beta: float`
    The ratio by which to weight precision to recall

##### `calculate`
```python
def calculate(
	predicted: Any[List]
	actual: Any[List]
)
```

Calculates F score with the class beta for the information retrieval pipeline's prediction against a positive set

###### Parameters
`predicted: List[Any]`
    The output of an information retrieval pipeline

`actual: List[Any]`
    The set of relevant instances for the input

##### `description`
```python
def description()
```

